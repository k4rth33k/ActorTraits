{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RgldLkFrrL1"
   },
   "source": [
    "# Finetuning BERT with Keras and tf.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sW8wMuz1rRZU"
   },
   "source": [
    "In this experiment we convert a pre-trained BERT model checkpoint into a trainable Keras layer, which we use to solve a sentence pair classification task.\n",
    "\n",
    "We achieve this using a tf.Module, which is a neat abstraction designed to handle pre-trained Tensorflow models.\n",
    "Exported modules can be easily integrated into other models, which facilitates experiments with powerful NN architectures.\n",
    "\n",
    "The plan for this experiment is:\n",
    "\n",
    "1.   getting a pre-trained BERT model checkpoint\n",
    "2.   defining the specification of the tf.Module\n",
    "3.   exporting the module\n",
    "4.   building the text preprocessing pipeline\n",
    "5.   implementing a custom Keras layer\n",
    "6.   training a Keras model to solve a sentence-pair classification task\n",
    "\n",
    "\n",
    "# What is in this guide?\n",
    "This guide is about integrating pre-trained Tensorflow models into Keras pipelines. It contains implementations of two things: a BERT tf.Module and a Keras layer built on top of it.\n",
    "# What does it take?\n",
    "For a reader familiar with TensorFlow it should take around 30 minutes to finish this guide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "7864AKjDrMsa",
    "outputId": "72d78560-ba36-40f0-b37a-d00ac53e7bd5"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/google-research/bert bert_repo\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from google.colab import auth, drive\n",
    "\n",
    "if not 'bert_repo' in sys.path:\n",
    "    sys.path.insert(0, 'bert_repo')\n",
    "\n",
    "from modeling import BertModel, BertConfig\n",
    "from tokenization import FullTokenizer, convert_to_unicode\n",
    "from extract_features import InputExample, convert_examples_to_features\n",
    "\n",
    "\n",
    "# get TF logger \n",
    "log = logging.getLogger('tensorflow')\n",
    "log.handlers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNM1WYKCpxtz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veuCjAVtRsuO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkqKH8ssRuFI"
   },
   "source": [
    "## Step 1: getting the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "9mxg4a3Y97Mt",
    "outputId": "41c4c62a-12f7-4f7f-9175-e3cfa6e30161"
   },
   "outputs": [],
   "source": [
    "# !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "# !unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0iVw5dkr9A_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlzqsoCERraJ"
   },
   "source": [
    "## Step 2: building a tf.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLQ8xyKLsEn5"
   },
   "source": [
    "**tf.Modules** are designed to provide a simple way to manipulate reusable parts of pre-trained machine learning models in Tensorflow. Google maintains a curated library of such modules at tf.Hub. In this guide however, we will build one by ourselves.\n",
    "\n",
    "To that end, we will need to implement a ***module_fn*** which will contain the full specification of the module inner workings. \n",
    "We begin by defining input placeholders. Then the BERT graph is created from a configuration file passed through ***config_path***. Then we model outputs are defined: the final encoder layer to seq_output and pooled *'**CLS**'* token representation to pool_output.\n",
    "\n",
    "Additionally, extra assets may be bundled with the module. In this example, we add a ***vocab_file*** containing the WordPiece vocabulary to the module assets. As a result, the vocabulary file will be exported with the module, which will make it self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOyKrgZRRqZe"
   },
   "outputs": [],
   "source": [
    "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
    "\n",
    "    def bert_module_fn(is_training):\n",
    "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
    "\n",
    "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
    "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
    "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "        config = BertConfig.from_json_file(config_path)\n",
    "        model = BertModel(config=config, is_training=is_training,\n",
    "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
    "          \n",
    "        seq_output = model.all_encoder_layers[-1]\n",
    "        pool_output = model.get_pooled_output()\n",
    "\n",
    "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
    "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
    "        lower_case = tf.constant(do_lower_case)\n",
    "\n",
    "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
    "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
    "        \n",
    "        input_map = {\"input_ids\": input_ids,\n",
    "                     \"input_mask\": input_mask,\n",
    "                     \"segment_ids\": token_type}\n",
    "        \n",
    "        output_map = {\"pooled_output\": pool_output,\n",
    "                      \"sequence_output\": seq_output}\n",
    "\n",
    "        output_info_map = {\"vocab_file\": vocab_file,\n",
    "                           \"do_lower_case\": lower_case}\n",
    "                \n",
    "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
    "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
    "\n",
    "    return bert_module_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5dJoFIGsfYI"
   },
   "source": [
    "Finally, we define signatures, which are particular transformations of inputs to outputs exposed to module consumers. One could think of it as a module interface with the outside world.\n",
    "\n",
    "Here we add two signatures to the module: one that takes raw text features as input and returns computed text representations as output. The other takes no inputs and returns the path to vocabulary file and lowercase flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocsQWnDiRqcx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubGdlgtTsjW1"
   },
   "source": [
    "## Step 3: exporting the module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Muib1AD4ssN2"
   },
   "source": [
    "Now that the module_fn is defined, we can use it to build and export the module. Passing the tags_and_args argument to create_module_spec will result in two graph variants being added to the module: for training with tags ***{\"train\"}*** and for inference with an empty set of tags. This allows to control dropout, which is disabled at inference time, and enabled during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "0fI0EDc3Qi6Z",
    "outputId": "e5d1681e-ddfb-4111-cd0c-44b24ec6e267"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "d4fShtfnSQbO",
    "outputId": "9a0df805-1034-44fd-e4cf-a72ca7a47442"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0427 18:13:05.699476 157700 deprecation_wrapper.py:119] From bert_repo\\modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0427 18:13:05.702468 157700 deprecation_wrapper.py:119] From bert_repo\\modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0427 18:13:05.744387 157700 deprecation_wrapper.py:119] From bert_repo\\modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W0427 18:13:07.877779 157700 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0427 18:13:07.895733 157700 deprecation.py:506] From bert_repo\\modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0427 18:13:07.930097 157700 deprecation.py:323] From bert_repo\\modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0427 18:13:10.115293 157700 deprecation.py:323] From c:\\users\\kartheek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_hub\\saved_model_lib.py:110: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"uncased_L-12_H-768_A-12\" #@param {type:\"string\"} ['uncased_L-12_H-768_A-12']\n",
    "\n",
    "config_path = \"./{}/bert_config.json\".format(MODEL_DIR)\n",
    "vocab_path = \"./{}/vocab.txt\".format(MODEL_DIR)\n",
    "\n",
    "tags_and_args = []\n",
    "for is_training in (True, False):\n",
    "  tags = set()\n",
    "  if is_training:\n",
    "    tags.add(\"train\")\n",
    "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
    "\n",
    "module_fn = build_module_fn(config_path, vocab_path)\n",
    "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
    "spec.export(\"bert-module\", \n",
    "            checkpoint_path=\"./{}/bert_model.ckpt\".format(MODEL_DIR), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "gQeLyV6vYMlp",
    "outputId": "512d343b-2caa-439a-e10c-b8cea5421122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\n",
      "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
     ]
    }
   ],
   "source": [
    "# !ls ./uncased_L-12_H-768_A-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xgf8hzNYMst"
   },
   "source": [
    "## Step 4: building the text preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWWY-PeRs9p1"
   },
   "source": [
    "The BERT model requires that text is represented as 3 matrices containing ***input_ids***, ***input_mask***, and ***segment_ids***. In this step we build a pipeline which takes a list of strings, and outputs these three matrices, as simple as that.\n",
    "\n",
    "First of all, raw input text is converted into ***InputExamples***. If the input text is a sentence pair, separated by a special '|||' sequence, the sentences are split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "la7nsfojtI-W"
   },
   "outputs": [],
   "source": [
    "def read_examples(str_list):\n",
    "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
    "    unique_id = 0\n",
    "    for s in str_list:\n",
    "        line = convert_to_unicode(s)\n",
    "        if not line:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        text_a = None\n",
    "        text_b = None\n",
    "        m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "        if m is None:\n",
    "            text_a = line\n",
    "        else:\n",
    "            text_a = m.group(1)\n",
    "            text_b = m.group(2)\n",
    "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n",
    "        unique_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DWCby58ctJFQ"
   },
   "source": [
    "***InputExamples*** are then tokenized and converted to ***InputFeatures*** using the ***convert_examples_to_features*** function from the original repository. However, we will require these features to be converted to np.arrays to use with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmsepoRxVsc2"
   },
   "outputs": [],
   "source": [
    "def features_to_arrays(features):\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_segment_ids.append(feature.input_type_ids)\n",
    "\n",
    "    return (np.array(all_input_ids, dtype='int32'), \n",
    "            np.array(all_input_mask, dtype='int32'), \n",
    "            np.array(all_segment_ids, dtype='int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNE1vP_ktgbn"
   },
   "source": [
    "Finally, let us put it all together in a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_061naESlic"
   },
   "outputs": [],
   "source": [
    "def build_preprocessor(voc_path, seq_len, lower=True):\n",
    "  tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
    "  \n",
    "  def strings_to_arrays(sents):\n",
    "  \n",
    "      sents = np.atleast_1d(sents).reshape((-1,))\n",
    "\n",
    "      examples = []\n",
    "      for example in read_examples(sents):\n",
    "          examples.append(example)\n",
    "\n",
    "      features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
    "      arrays = features_to_arrays(features)\n",
    "      return arrays\n",
    "  \n",
    "  return strings_to_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HDop0a4tixD"
   },
   "source": [
    "All done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LycbxDVlalim"
   },
   "source": [
    "## Step 5: implementing a BERT Keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIIv5wCKUkgX"
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
    "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
    "                 tune_embeddings=False, trainable=True, **kwargs):\n",
    "\n",
    "        self.trainable = trainable\n",
    "        self.n_tune_layers = n_tune_layers\n",
    "        self.tune_embeddings = tune_embeddings\n",
    "        self.do_preprocessing = do_preprocessing\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.seq_len = seq_len\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "\n",
    "        self.var_per_encoder = 16\n",
    "        if self.pooling not in [\"cls\", \"mean\", None]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
    "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
    "\n",
    "        trainable_layers = []\n",
    "        if self.tune_embeddings:\n",
    "            trainable_layers.append(\"embeddings\")\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            trainable_layers.append(\"pooler\")\n",
    "\n",
    "        if self.n_tune_layers > 0:\n",
    "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
    "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
    "            for i in range(self.n_tune_layers):\n",
    "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
    "        \n",
    "        # Add module variables to layer's trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if any([l in var.name for l in trainable_layers]):\n",
    "                self._trainable_weights.append(var)\n",
    "            else:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"*** TRAINABLE VARS *** \")\n",
    "            for var in self._trainable_weights:\n",
    "                print(var)\n",
    "\n",
    "        self.build_preprocessor()\n",
    "        self.initialize_module()\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def build_abspath(self, path):\n",
    "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
    "          return path\n",
    "        else:\n",
    "          return os.path.abspath(path)\n",
    "\n",
    "    def build_preprocessor(self):\n",
    "        sess = tf.keras.backend.get_session()\n",
    "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
    "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                              tokenization_info[\"do_lower_case\"]])\n",
    "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
    "\n",
    "    def initialize_module(self):\n",
    "        sess = tf.keras.backend.get_session()\n",
    "        \n",
    "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
    "                                     for var in self.bert.variables])\n",
    "\n",
    "        uninitialized = []\n",
    "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
    "            if not is_initialized:\n",
    "                uninitialized.append(var)\n",
    "\n",
    "        if len(uninitialized):\n",
    "            sess.run(tf.variables_initializer(uninitialized))\n",
    "\n",
    "    def call(self, input):\n",
    "\n",
    "        if self.do_preprocessing:\n",
    "          input = tf.numpy_function(self.preprocessor, \n",
    "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
    "                                    name='preprocessor')\n",
    "          for feature in input:\n",
    "            feature.set_shape((None, self.seq_len))\n",
    "        \n",
    "        input_ids, input_mask, segment_ids = input\n",
    "        \n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "        \n",
    "        if self.pooling == \"cls\":\n",
    "            pooled = output[\"pooled_output\"]\n",
    "        else:\n",
    "            result = output[\"sequence_output\"]\n",
    "            \n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            \n",
    "            if self.pooling == \"mean\":\n",
    "              pooled = masked_reduce_mean(result, input_mask)\n",
    "            else:\n",
    "              pooled = mul_mask(result, input_mask)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def get_config(self):\n",
    "        config_dict = {\n",
    "            \"bert_path\": self.bert_path, \n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"pooling\": self.pooling,\n",
    "            \"n_tune_layers\": self.n_tune_layers,\n",
    "            \"tune_embeddings\": self.tune_embeddings,\n",
    "            \"do_preprocessing\": self.do_preprocessing,\n",
    "            \"verbose\": self.verbose\n",
    "        }\n",
    "        super(BertLayer, self).get_config()\n",
    "        return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS8FTjkh_maW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIjYf1EL_nQA"
   },
   "source": [
    "## Step 6: sentence pair classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJF9vqwgtycK"
   },
   "source": [
    "Now let us try the layer on a real-world dataset. For this part we will use the [Quora Question Pairs](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs) dataset which consists of over 400,000 potential question duplicate pairs labeled for semantic equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxRGaEW1g70T"
   },
   "outputs": [],
   "source": [
    "# !wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv -O quora_train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAbLo0y7g7xf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6BanaWyuKQ-"
   },
   "source": [
    "We join the question pairs with the \"|||\" sequence and split them into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtqfzXJYADug"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../personality_data/data.csv\")\n",
    "\n",
    "texts = df.text\n",
    "df.drop(['text', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "labels = df.values\n",
    "\n",
    "# texts = []\n",
    "# delimiter = \" ||| \"\n",
    "# for q1, q2 in zip(df.question1.tolist(), df.question2.tolist()):\n",
    "#   texts.append(delimiter.join((str(q1), str(q2))))\n",
    "\n",
    "texts = np.array(texts)\n",
    "\n",
    "trX, tsX, trY, tsY = train_test_split(texts, labels, shuffle=True, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cAGR', 'cCON', 'cEXT', 'cNEU', 'cOPN'], dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K4rO0RKgdzsa",
    "outputId": "d6fd51c7-5c1c-4e9c-d0a3-4c7f3469252d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3178,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFuVy0K6bnE0"
   },
   "source": [
    "Building and training a sentence-pair classification model is straighforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "5BQLsvYzaysD",
    "outputId": "bff3e09a-32e6-4c0d-ef78-4451f23ba4cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0427 18:13:57.251271 157700 deprecation.py:506] From c:\\users\\kartheek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=48, tune_embeddings=False,\n",
    "                    pooling='cls', n_tune_layers=3, verbose=False)\n",
    "# encoder.trainable = False\n",
    "\n",
    "h1 = tf.keras.layers.Dense(128, activation='relu')(encoder(inp))\n",
    "h2 = tf.keras.layers.Dense(64, activation='relu')(h1)\n",
    "h3 = tf.keras.layers.Dense(32, activation='relu')(h2)\n",
    "h4 = tf.keras.layers.Dense(16, activation='relu')(h3)\n",
    "\n",
    "\n",
    "pred = tf.keras.layers.Dense(5, activation='sigmoid')(h4)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])\n",
    "\n",
    "print(encoder.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "wxuy1OMCbGTw",
    "outputId": "1ace4917-1722-4258-ece8-277bd53f06e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0427 18:14:08.401031 157700 deprecation.py:323] From c:\\users\\kartheek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "bert_layer (BertLayer)       (None, 768)               109482240 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 109,591,621\n",
      "Trainable params: 21,963,589\n",
      "Non-trainable params: 87,628,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ),\n",
    "      loss=\"binary_crossentropy\",\n",
    "      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0V7AbjTBifTl"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lbagCxaieoG"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "z6svZ93niek0",
    "outputId": "74e6a4f5-c8f5-4fcd-cba7-dadd3f6007ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2860 samples, validate on 318 samples\n",
      "Epoch 1/5\n",
      "2860/2860 [==============================] - 66s 23ms/sample - loss: 0.5530 - acc: 0.6697 - val_loss: 0.5733 - val_acc: 0.6516\n",
      "Epoch 2/5\n",
      "2860/2860 [==============================] - 71s 25ms/sample - loss: 0.5469 - acc: 0.6766 - val_loss: 0.5728 - val_acc: 0.6541\n",
      "Epoch 3/5\n",
      "2860/2860 [==============================] - 69s 24ms/sample - loss: 0.5399 - acc: 0.6850 - val_loss: 0.5740 - val_acc: 0.6503\n",
      "Epoch 4/5\n",
      "2860/2860 [==============================] - 73s 25ms/sample - loss: 0.5340 - acc: 0.6940 - val_loss: 0.5737 - val_acc: 0.6535\n",
      "Epoch 5/5\n",
      "2860/2860 [==============================] - 72s 25ms/sample - loss: 0.5288 - acc: 0.6972 - val_loss: 0.5724 - val_acc: 0.6591\n"
     ]
    }
   ],
   "source": [
    "saver = keras.callbacks.ModelCheckpoint(\"bert_tuned_3.hdf5\")\n",
    "\n",
    "hist = model.fit(trX, trY, validation_data=[tsX, tsY], batch_size=128, epochs=5, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rivD55Qc85D"
   },
   "source": [
    "## Step 7: saving and restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnJNgkArdBAs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68055534, 0.31686515, 0.5492288 , 0.19271302, 0.94487745],\n",
       "       [0.38152623, 0.4261461 , 0.43594748, 0.49153474, 0.06401667],\n",
       "       [0.31303954, 0.34893757, 0.35519063, 0.5158832 , 0.01817501],\n",
       "       [0.38691646, 0.428504  , 0.4405269 , 0.47391734, 0.06708089],\n",
       "       [0.37010187, 0.37845913, 0.3868185 , 0.47120234, 0.02930057],\n",
       "       [0.6074951 , 0.45277885, 0.68986475, 0.44066992, 0.961125  ],\n",
       "       [0.402719  , 0.39206484, 0.44025055, 0.39353317, 0.04253593],\n",
       "       [0.56434953, 0.49045318, 0.69330794, 0.6118047 , 0.95182693],\n",
       "       [0.54166317, 0.4677839 , 0.6129382 , 0.704936  , 0.9308933 ],\n",
       "       [0.42561972, 0.4053361 , 0.41189033, 0.6060305 , 0.22798362]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(trX[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ix02jwj_u_64"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(model.to_json(), open(\"model.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgWsbzSaeSC6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0428 18:38:17.867848 185548 deprecation.py:506] From c:\\users\\kartheek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0428 18:38:17.871848 185548 deprecation.py:506] From c:\\users\\kartheek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0428 18:38:17.875849 185548 deprecation.py:506] From c:\\users\\kartheek\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0428 18:38:28.997657 185548 deprecation_wrapper.py:119] From bert_repo\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.model_from_json(json.load(open(\"model.json\")), \n",
    "                                        custom_objects={\"BertLayer\": BertLayer})\n",
    "\n",
    "model.load_weights(\"bert_tuned_3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7H2AY9MsvUx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68055534, 0.31686515, 0.5492288 , 0.19271302, 0.94487745],\n",
       "       [0.38152623, 0.4261461 , 0.43594748, 0.49153474, 0.06401667],\n",
       "       [0.31303954, 0.34893757, 0.35519063, 0.5158832 , 0.01817501],\n",
       "       [0.38691646, 0.428504  , 0.4405269 , 0.47391734, 0.06708089],\n",
       "       [0.37010187, 0.37845913, 0.3868185 , 0.47120234, 0.02930057],\n",
       "       [0.6074951 , 0.45277885, 0.68986475, 0.44066992, 0.961125  ],\n",
       "       [0.402719  , 0.39206484, 0.44025055, 0.39353317, 0.04253593],\n",
       "       [0.56434953, 0.49045318, 0.69330794, 0.6118047 , 0.95182693],\n",
       "       [0.54166317, 0.4677839 , 0.6129382 , 0.704936  , 0.9308933 ],\n",
       "       [0.42561972, 0.4053361 , 0.41189033, 0.6060305 , 0.22798362]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(trX[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlXawIFPofGe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I'm not really sure what exactly I'm supposed to do with this. There is some guy in my room that keeps trying to talk to me and I don't know him. I wonder why he's in here. Theres a really old song playing on the radio. I havent heard it in a while. it reminds me of 6th grade. Every song does that, takes me back to a different place in time. Now this song reminds me of my friends Jason and Doug. I wonder what they're doing right now. I miss my old friends. I'm really excited about this weekend. Miami is going to be a blast. I just hope that everything works out. I really want to suprise John, but if I have to tell him I'm coming I guess I will. I really don't want to. That guy won't shut up. I'm thinking I might just take a cab from the airport to the university. My dad said theres a tropical storm. I sure hope it disentigrates before I get there. I hope everything works out with John, though. I think this weekend is going to be the deciding factor of our relationship. Things are so tense every time we talk. I want to go back to that last night at his house. . it was so amazing. We stayed up the enitre night watching the meteor shower and talking out on his balcony. one of those moments I will never forget even when I'm old. I wonder what I will be like when I'm old. I want to be a cool grandma. I want to live in a big old house on some land but drive a really badass car and cook good food for m grandkids and spoil them. I guess b efore I  have grandkids, I have to have kids. which I don't want to do. I was watching the learning channel and MAN having those babies has to hurt. I could just have a c section. Thats what my aunt did. But then she died of cancer. Maybe I shouldnt have one. This cough is getting really old. I always get colds. Last time I had one was before me and John met though. HE brought me flowers for the first time. White dasies on the red tahoe outside of my room. I miss that. The note said he didnt have any soup. flowers would have to do. I love that. If I could right now, i would fly to miami and marry this boy. i remember the day my sister got married. It was the most beautiful thing i've seen. OF COURSE I WOULD DO IT DIFFERNETLY IF IT WAS MY WEDDING&gt; OK NOW MY COMPUTER IS FREAKING OUT&gt; UMMM I don't KNOW WHAT I DID BUT IT won't QUIT! OK THIS IS GOING TO BOTHER ME SO MUCH&gt; I HATE THAT WHEN SOMETHING HAPPENS WITH YOUR COMPUTER AND YOU don't KNOW HOW TO FIX IT&gt; SOMETIMES THESE THINGS ARE JUST TOO SMART FOR THEIR OWN GOOD&gt; MY EYE ITCHES BUT I can't ITCH IT&gt; WIERD&gt; I WONDER WHY YOU can't ITCH INSIDE&lt; BUT IT ITCHES ANYWAYS&gt; HMM&gt; I WONDER WHY YOUR FOOT WILL JUST FALL ASLEEP ALL OF A SUDDEN AND YOU can't WAKE IT UP&gt; WHY DO THEY SAY IT FELL ASLEEP? IT SMELLS LIKE PEANUT BUTTER IN MY ROOM BECAUSE MY ROOMATE IS MAKING A PB&J SANDWICH&gt; I  M HUNGRY NOW TOO&gt; I COULDNT EAT AT OLIVE GARDEN EARLIER BECAUSE I WAS COUGHING TOO MUCH&lt; BUT NOW ITS A LITTLE BETTER&gt; I SHOULD TRY TO EAT SOMETHING&gt; I WISH THERE WAS JUST A PILL THAT YOU COULD TAKE THAT WOULD BE FOOD&gt; IT WOULD BE A UNIVERSAL THING&gt; FOOD TAKES UP SO MUCH OF OUR TIME AND LIVES&gt; WE WOULD LIVE CHEAPER&lt; HAVE MORE TIME TO DO OTHER THINGS&lt; AND PROBALY WOULDNT HAVE SO MANY OVERWEIGHT PEOPLE&gt; THATS A GOOD IDEA&gt; I THINK IM GOING TO COME UP WITH ONE OF THOSE&gt;&gt;&gt;\",\n",
       "       \"Yeah, oh, Princess Leia and the gold bikini, every guy our age loved that.  Um, um. It's huge. Yeah, that's the moment, when-when, you know she stopped being a princess, and became, like, a woman, you know.  Oh, yeah, um-mm. Oh!  No it's just that I got this new pager and I have it on vibrate. See ya!  Really!  Did you ever do the-the Leia thing?  Really! That-that great huh?\",\n",
       "       \"Today, i started the day with a serve headache. All day long this feeling has been with me from my first class to work to homework. In the early months i was told that i had a tumor near my pitutary gland. Ever since then I was put on some medication to help dissolve this tumor. Friday, i was schedule to have a MRI done, well today the result came back. As the doctor reported, i the medicine is working well. I just have to keep taking the medication. What a big relief. I felt great that i will not have to have surgery to remove the tumor. But lately, i have had headache that hurt realyy bad. When i toldmy mother, see thought i sgould get me eyes checked. She thinks that i need glasses. Who knows? I hope not. MAn, my brother is in the 5th grade and they required him to buy a recorder. Yes, he does not know how to play it. So it just sounds like a bunch of noise. This is what I hear in background as well as the loud tv that is showing commericals. I feel sleepy. Actually i don't know if there is time in which i am not sleepy. I work alot as a server and then get to come to do some homework before i fall asleep. So not much gets done. I wish there was just a day were i could relax, do something fun and not have to worry about what is due tomorrow, next week, in a couple of months get my drift. i know we just had a summer vaction. Well, i took summer school for the second half. so it feels like i have been in school for a couple of months. College is so much different from high school. Alot of responsibility is on the student. It is very easy to miess up. So far, i try to take one think at a time. It is hard to jugle 10 things at once, even if they are not school related. I deal with school, work ,friends, family and down time for myself which is usually the gym. One thing that i do miss is dealing with the boyfriend. After 2 1/2 yrs, i decided to break it off. I felt that i needed some breathing room. But there have been days were i miss  him. At those times, i just want him back but then i see that i can't just go and come when i want too. it is not fair to him. Lately i have been feeling lonely, very lonely. Especially since i don't see my friends as often because i live off campus, at home. Which means that i have some what of a curfew. Plus to travel from my house to campus is 15-20 way. Any way i really don't have time. This guy took very good care of me. I know, so why did i leave. Well he treated our relationship as we were, married. I could not even talk to friends with =out him geeting jealous or us getting a arguement. He wanted to spend all his free time with me and expected me to do the same. Well, i appreicate my friends and he did'n see that.\",\n",
       "       ...,\n",
       "       \"It comes easy just to type out what I am thinking, I am used to keeping a thourogh journal. It didnt necessarily have written sections like a   dear diary   but had many ideas and thoughts expressed in art. I really do love art. The communications school has a fantastic creative program which I hope to become a part of. It a rigorous comittment that i feel fully qualified for. That is my own concieded thought. Anyways, college has been everything I expected for and more. My professors are interesting and the social scene is very entertaining. I still find myself getting sad at nights because I am not with some of my closest friends. Lauren Nagler, Stephanie Miller, Adam Gutmann. . some kids that are really special to me and its really sad not seeing them everyday. I also see myself getting sad that i can not see my parents everyday. I talk to them quite alot but its not the same. It kind of scares me too since my mom is going through alot, and batteling cancer, and my inconsiderate brother takes no responsibility in the family. It was supposed to be my year at college my time to be   off duty   and my brother thinks its appropriate to go and work in the Netherlands. I have such ill feelings for that boy. Besides that. I mean life goes on. You can't get stuck in a rut. you got to live each day as it comes. There has been some drama in my   new   life here at UT. It is all a bunch of dumb girls who think they know what they are talking about and they really dont. I am a pretty chill, I am not a competer. ill quit before the race begins just because i don't care about things that much. And especially with guys, I am not the type of girl to be a homewrecker. And for Petes sake I am only 19- a freshman- its not like the people we have found this week are going to be our soulmates for life. Anat- the girl whose giving me the worst vibes in the world thinks she can call dibs on every guy- who is she to do that??? whatever- again i don't compete- take them all and be happy- i am too worried about my grades and other things that are improtant to me. This writing assignment is actually pretty cool. Its like a way to vent out whats on your mind and get a grade for it. It is really hot in my dorm room right now, and the cd i was listening to is over, Lynard Skynard, what a good cd. I think my car phone is broken too and its like the fourth one i have gone through- I got these cute pictures today at tops photo. i am really getting into the swing of things here at UT and its really comforting. I came with alot of friends. some who i didnt reallly want to hang out with some I did- I have met alot of cool cool peeps and i am definatly having a good time. Its really hard to find something to talk about with there is one minute left----\",\n",
       "       'For some reason today I feel more home sick than I have yet. When I first arrived to my dorm room three weeks ago, it didn\\'t feel very real to me. Of course I cried when my parents left, but it wasn\\'t because I was feeling all the things that I thought I would. At the time I think it was more knowing that I would soon feel that way once the excitement wore off. Now, dorm life is very real and I no longer feel like I am at summer camp. The first two weeks were nothing but meeting new people and going new places and now I feel the overwhelming feeling of college work and homesickness. It is rather silly I guess for me to say \"homesick\" because I am only thirty miles away from home, but I have discovered that it doesn\\'t really matter what your distance from home is. Distance would make it harder, but for all of us it is the fact that we are no longer living with our families; we will never be living under our parents permanently. In a way I guess I am supposed to be really excited about that and I think I am, but I also really miss being a bigger part of their lives. So much happens on a daily basis and I feel like I am missing something. I also wonder if it is really weird for me to feel this way. So many of the other students seem to be having the time of their lives, but I wonder if they have ever felt the same way. I went to eat with my mom and sisters last night because my older sister was in town from College Station. After we ate I couldn\\'t fight the tears that were welling up in my eyes. I miss old times like that and I miss the way we could just laugh together and have fun. I know that soon I will be adjusted to my new life and I can look back on this and think what a dork I was. It is just that time of adjustment and waiting until this place feels like home. I want so badly for it to feel like home and I want to be happy here. I really have no doubt that it soon will be a place where I feel comfortable, I just hope that soon comes really soon! My mom keeps sending emails telling me to have fun. It isn\\'t that easy to just make friends and go have fun. I never thought that it would be so hard to meet people, but with such a huge population it can\\'t be easy. I think that I am probably complaining too much because I really am not unhappy by any means but I just expected something different. High expectations are dangerous because nothing can ever be as good as you dream it to be. Oh well, hook \\'em horns!',\n",
       "       \"Multiple, so many paper cuts.  Why hasn't he called Rachel? Why? Why? I don't understand. Why? He said he'll call. Why? Why? Chandler I'm telling you she has flipped out, she's gone crazy!  Oh, well give me the phone then.  Look, you can't call somebody after this long just to say, 'In case you didn't notice, I don't like you!'  Nooo!! She's really dull! And she gets this gross mascara goop thing in the corner of her eye!  Call her! Call her now!  Come on, this isn't funny. She thinks it's my fault that you haven't called her. You have to call her!  Well then you're going to have to take her out again.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnJa8oWJ2u8H"
   },
   "source": [
    "In some cases (e.g. when serving), one might want to optimize the trained model for maximum inference throughput. In TensorFlow this can be achieved by \"freezing\" the model. \n",
    "\n",
    "During \"freezing\" the model variables are replaced by constants, and the nodes required for training are pruned from the computational graph. The resulting graph becomes more lightweight, requires less RAM and achieves better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pg5crVOofJo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
    "\n",
    "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes a Keras model into a pruned computation graph.\n",
    "\n",
    "    @param model The Keras model to be freezed.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    \n",
    "    sess = tf.keras.backend.get_session()\n",
    "    graph = sess.graph\n",
    "    \n",
    "    with graph.as_default():\n",
    "\n",
    "        input_tensors = model.inputs\n",
    "        output_tensors = model.outputs\n",
    "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
    "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
    "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
    "        \n",
    "        tmp_g = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in tmp_g.node:\n",
    "                node.device = \"\"\n",
    "        \n",
    "        tmp_g = optimize_for_inference(\n",
    "            tmp_g, input_ops, output_ops, dtypes, False)\n",
    "        \n",
    "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
    "        \n",
    "        if export_path is not None:\n",
    "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
    "                f.write(tmp_g.SerializeToString())\n",
    "        \n",
    "        return tmp_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPpYP_ge55ha"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjbufyTp5-90"
   },
   "source": [
    "We freeze our trained model and write the serialized graph to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gMUrFpgofMf"
   },
   "outputs": [],
   "source": [
    "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6cljim3u_3g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI77AK8w6no7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXePoLYV6n27"
   },
   "source": [
    "Now let's restore the frozen graph and do some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShmlWdXLcH80"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/gaphex/bert_experimental/\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"/content/bert_experimental\")\n",
    "\n",
    "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
    "from bert_experimental.finetuning.graph_ops import load_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uz7gbQp0douz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khWpzTjPvUZv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avKRLyiXvrqa"
   },
   "outputs": [],
   "source": [
    "restored_graph = load_graph(\"frozen_graph.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZXDHbHd7eCx"
   },
   "source": [
    "To run inference we need to get the handles for input and output tensors of the graph. This part a little tricky: we retrieve a list of all operations in the restored graph and then manually get the names of relevant ops. The list is sorted, so in this case it is enough to take the first and the last operation.\n",
    "\n",
    " To get the Tensor name we append **\":0\"** to the op name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8bT-YXrgo6j"
   },
   "outputs": [],
   "source": [
    "graph_ops = restored_graph.get_operations()\n",
    "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
    "print(input_op, output_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHr2ZQGfg3y2"
   },
   "outputs": [],
   "source": [
    "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
    "y = restored_graph.get_tensor_by_name(output_op + ':0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDsG0Pkw88f-"
   },
   "source": [
    "The preprocessing function we injected into the Keras layer is not serializable and was not restored in the new graph. No worries though - we can simply define it again with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPoapr4e86Nc"
   },
   "outputs": [],
   "source": [
    "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
    "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQFQOML7ivdg"
   },
   "outputs": [],
   "source": [
    "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKDhrawB9n1G"
   },
   "source": [
    "Finally, we can get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRf75n6ECUa7"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=restored_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IbB4pjlCcmY"
   },
   "outputs": [],
   "source": [
    "trX[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw55NB6YseBK"
   },
   "outputs": [],
   "source": [
    "y_out = sess.run(y, feed_dict={\n",
    "        x: trX[:10].reshape((-1,1))\n",
    "    })\n",
    "\n",
    "y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7GX13v2iyzZ"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6Jh5rpZlFx-"
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('../Actor Quotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qxOCE1vlI0H"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Aamir Khan</td>\n",
       "      <td>The journey that I have undertaken, meeting pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aasif Mandvi</td>\n",
       "      <td>If you don't acknowledge differences, it's as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Abhimanyu Singh</td>\n",
       "      <td>Karthi is such a wonderful human being. He is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Abhishek Bachchan</td>\n",
       "      <td>You can work really hard on your physicality, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aditya Roy Kapur</td>\n",
       "      <td>I am not good with PR or in projecting a certa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              Actor  \\\n",
       "0           0         Aamir Khan   \n",
       "1           1       Aasif Mandvi   \n",
       "2           2    Abhimanyu Singh   \n",
       "3           3  Abhishek Bachchan   \n",
       "4           4   Aditya Roy Kapur   \n",
       "\n",
       "                                              Quotes  \n",
       "0  The journey that I have undertaken, meeting pe...  \n",
       "1  If you don't acknowledge differences, it's as ...  \n",
       "2  Karthi is such a wonderful human being. He is ...  \n",
       "3  You can work really hard on your physicality, ...  \n",
       "4  I am not good with PR or in projecting a certa...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    266\n",
       "Actor         266\n",
       "Quotes        266\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IruxM0xkrBja"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = np.array(text)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266, 5)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(predictions > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = pd.DataFrame(columns=['Actor', 'cAGR', 'cCON', 'cEXT', 'cNEU', 'cOPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Actor', 'Quotes'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actor</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Actor, cAGR, cCON, cEXT, cNEU, cOPN]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for actor, traits_ in zip(df_new['Actor'], predictions):\n",
    "    _ = pd.DataFrame([[actor] + traits_.tolist()], columns=['Actor', 'cAGR', 'cCON', 'cEXT', 'cNEU', 'cOPN'])\n",
    "    \n",
    "    traits = traits.append(_, ignore_index=True)\n",
    "#     print(actor, traits)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266, 6)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits.to_csv('Actor Traits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Finetuning BERT with Keras and tf.Module.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
